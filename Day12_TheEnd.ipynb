{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "Finally, we're going to fit all of our code together! Most of this will be code that you wrote. We're going to glue it all together and provide you with a few final pieces --- a function to train a classifier, a scorer. Ask lots of questions, and be really proud of how much you've done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_body_path = \"train_bodies.csv\"\n",
    "if not os.path.exists(train_body_path):\n",
    "    print(\"Check location for train_bodies\")\n",
    "test_body_path = \"competition_test_bodies.csv\"\n",
    "if not os.path.exists(test_body_path):\n",
    "    print(\"Check location for test_bodies\")\n",
    "train_stance_path = \"train_stances.csv\"\n",
    "if not os.path.exists(train_stance_path):\n",
    "    print(\"Check location for train_stances\")\n",
    "test_headline_path = \"competition_test_stances.csv\"\n",
    "if not os.path.exists(test_headline_path):\n",
    "    print(\"Check location for test_stances_unlabeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = {}\n",
    "\n",
    "def initialize():\n",
    "    global word2vec\n",
    "    if len(word2vec) == 0:\n",
    "        print('loading word2vec...')\n",
    "        word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        for word in word_vectors.vocab:\n",
    "            word2vec[lemmatizer.lemmatize(clean(word))] = word_vectors[word]\n",
    "    print('word2vec loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global word2vec\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we had before\n",
    "\n",
    "Here's our cleaning and loading code! The only change we've made is adding a function for putting test data into a list of tuples, much like our training stances. You don't have to do anything anywhere, but read through it! Be impressed by what you've done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's the code we've written so far!\n",
    "# You don't have to do anything, but read it over and make sure you remember what each function is doing\n",
    "\n",
    "def clean(s):\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "def w_tokenize(s):\n",
    "    return nltk.word_tokenize(s)\n",
    "\n",
    "def s_tokenize(p):\n",
    "    return nltk.sent_tokenize(p)\n",
    "\n",
    "def lemmatize(word_tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokens]\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    return [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "def w_super_clean(s):\n",
    "    return remove_stopwords(lemmatize(w_tokenize(clean(s))))\n",
    "\n",
    "def s_super_clean(p):\n",
    "    sentences = s_tokenize(p)\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        clean_sentences.append(\" \".join(remove_stopwords(lemmatize(w_tokenize(clean(s))))))\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's our load body function from before\n",
    "# Again, you don't need to do anything, but read through and ask if any lines confuse you\n",
    "def load_body(filename):\n",
    "    id2body = {} \n",
    "    id2body_sentences = {} \n",
    "    \n",
    "    # These lines open the file and read in each row\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            \n",
    "            # This line gets the Body ID for this row\n",
    "            id = row['Body ID']\n",
    "            # This line gets the article body\n",
    "            body = str(row['articleBody'])\n",
    "            # This line strips leading and trailing spaces from the body\n",
    "            body = body.strip()\n",
    "            \n",
    "            # Cleaning words and sentences\n",
    "            body_words = w_super_clean(body) \n",
    "            body_sentences = s_super_clean(body)\n",
    "            \n",
    "            # Adding to the two dictionaries\n",
    "            id2body[id] = body_words\n",
    "            id2body_sentences[id] = body_sentences\n",
    "    \n",
    "    return id2body, id2body_sentences\n",
    "\n",
    "# Load in headlines - body pairs and their stances for training\n",
    "def load_stance(filename):\n",
    "    stances = []\n",
    "    \n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            clean_title = w_super_clean(row['Headline'])\n",
    "            stances.append((clean_title, row['Body ID'], row['Stance'].strip()))\n",
    "    return stances\n",
    "\n",
    "# Load in headlines - body pairs without stances for testing\n",
    "def load_test(filename):\n",
    "    test = []\n",
    "    \n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            clean_title = w_super_clean(row['Headline'])\n",
    "            test.append((clean_title, row['Body ID']))\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our counting dictionary function\n",
    "def dictionary_count(count_items, count_dictionary):\n",
    "    for item in count_items:        \n",
    "        if item in count_dictionary:\n",
    "            count_dictionary[item] += 1\n",
    "        else: \n",
    "            count_dictionary[item] = 1\n",
    "    \n",
    "    return count_dictionary\n",
    "\n",
    "# Our duplicate-eliminating function\n",
    "def elim_dupes(items):\n",
    "    new_list = []\n",
    "    for item in items:\n",
    "        if item not in new_list:\n",
    "            new_list.append(item)\n",
    "    return new_list\n",
    "\n",
    "# Our function for making the idf!\n",
    "def prepare_idf(corpus):\n",
    "    docs_containing = {}\n",
    "    idf = {}\n",
    "    for (body_id, body) in corpus.items():\n",
    "        no_dupes = elim_dupes(body)\n",
    "        docs_containing = dictionary_count(no_dupes, docs_containing)\n",
    "    for word in docs_containing:\n",
    "        idf[word] = log(len(corpus) / docs_containing[word])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to run this once so we can test our word overlap function\n",
    "id2body, id2body_sentences = load_body(train_body_path) \n",
    "idf = prepare_idf(id2body)\n",
    "train_stances = load_stance(train_stance_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating overlap\n",
    "\n",
    "Here's the function we wrote Monday to calculate word overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function returns an array representing the overlap between title and body\n",
    "def get_word_overlaps(title, body, trim_len=None):\n",
    "    words_in_body = {}\n",
    "    words_in_title = {}\n",
    "    \n",
    "    #TODO: use dictionary_count to count words in body\n",
    "    words_in_body = dictionary_count(body[:trim_len], words_in_body)\n",
    "    #TODO: use dictionary_count to count words in title\n",
    "    words_in_title = dictionary_count(title, words_in_title)\n",
    "\n",
    "    maximum_scaled = 0.0\n",
    "    # TODO: get the maximum word overlap count by taking the length of the title\n",
    "    maximum_count = len(title)\n",
    "    \n",
    "    for (word, title_word_count) in words_in_title.items():\n",
    "        # TODO: calculate maximum possible scaled overlap by multiplying the count of each word times its idf\n",
    "        if word in idf:\n",
    "            maximum_scaled += title_word_count * idf[word]\n",
    "        else: \n",
    "            maximum_scaled += title_word_count\n",
    "\n",
    "    overlap_scaled = 0\n",
    "    overlap_count = 0\n",
    "    \n",
    "    for (word, title_word_count) in words_in_title.items():\n",
    "        # TODO: check if this word is in the body\n",
    "        if word in words_in_body:\n",
    "            # TODO: get the number of overlaps by finding the minumum between how many times it appears in the title and the body\n",
    "            tf = min(title_word_count, words_in_body[word])\n",
    "            \n",
    "            # TODO: add the number of overlaps to the overlap count\n",
    "            overlap_count += tf\n",
    "            # TODO: scale the number of overlaps by the idf of the word and add it to the scaled overlap count\n",
    "            if word in idf:\n",
    "                overlap_scaled += tf * idf[word]\n",
    "            else:\n",
    "                overlap_scaled += tf\n",
    "\n",
    "    # TODO: divide the scaled overlap by the maximum possible scaled overlap\n",
    "    scaled_over_max = overlap_scaled / maximum_scaled\n",
    "    # TODO: divide the overlap count by the maximum possible overlap count\n",
    "    count_over_max = overlap_count / maximum_count\n",
    "    \n",
    "    # TODO: return a vector of overlap_count, count_over_max, overlap_scaled, and scaled_over_max\n",
    "    return [overlap_count, count_over_max, overlap_scaled, scaled_over_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity\n",
    "\n",
    "And here's a function to calculate semantic similarity. Do you remember this function from yesterday? Do you remember what each line does? What does the output look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2vector(sentence, word2vec):\n",
    "    vector = np.array([0.0] * 300)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            vector += word2vec[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "        vector /= np.linalg.norm(vector)\n",
    "    return vector\n",
    "\n",
    "def get_semantic_similarities(title, body_sentences):\n",
    "    title_vector = sentence2vector(title, word2vec)\n",
    "    max_sim = -1\n",
    "    best_vector = np.array([0.0] * 300)\n",
    "\n",
    "    supports = []\n",
    "    for sub_body in body_sentences:\n",
    "        sub_body_vector = sentence2vector(sub_body, word2vec)\n",
    "        similarity = 0\n",
    "        for i in range(300):\n",
    "            similarity += title_vector[i] * sub_body_vector[i]\n",
    "        if similarity > max_sim:\n",
    "            max_sim = similarity\n",
    "            best_vector = sub_body_vector\n",
    "\n",
    "        supports.append(similarity)\n",
    "\n",
    "    features = [max(supports), min(supports)]\n",
    "\n",
    "    for v in best_vector:\n",
    "        features.append(v)\n",
    "    for v in title_vector:\n",
    "        features.append(v)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an updated version of extract_features. We have get_word_overlap like from before, but we're calling it on a trimmed article version as well (like in the challenge part from Monday) and incorporating semantic similarity. Take a peak at the output of the test code beneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(title, body, body_sentences):\n",
    "    # TODO: make an empty list to hold the features\n",
    "    features = []\n",
    "    \n",
    "    # TODO: get the word overlap and append it to the feature list\n",
    "    features += get_word_overlaps(title, body)\n",
    "    features += get_word_overlaps(title, body, len(title)*4)\n",
    "    \n",
    "    features += get_semantic_similarities(title, body_sentences)\n",
    "    \n",
    "    # TODO: return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out extract features! We'll pair headlines and bodies from the first examples. Does the feature vector look right to you? Why is it so long? Take a look at get_semantic_similarities and try to figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['police', 'find', 'mass', 'graf', 'least', '15', 'body', 'near', 'mexico', 'town', '43', 'student', 'disappeared', 'police', 'clash']\n",
      "['danny', 'boyle', 'directing', 'untitled', 'film', 'seth', 'rogen', 'eyed', 'play', 'apple', 'co', 'founder', 'steve', 'wozniak', 'sony', 'steve', 'job', 'biopic', 'danny', 'boyle', 'directing', 'untitled', 'film', 'based', 'walter']\n",
      "[0, 0.0, 0, 0.0, 0, 0.0, 0, 0.0, 0.65824749307095565, 0.0, 0.021088778935321763, -0.013503400192135808, 0.011792165074367176, 0.0083971184785377427, 0.0051117664415396315, -0.0048224470426220184, -0.079734232450517598, 0.02734136878870715, -0.035052896274549127, 0.054551104106816711, -0.046932816995981362, 0.025246202715028889, -0.073890254828778881, 0.047495001609991892, 0.02221314816334282, 0.005418911206267335, -0.0024461886619384933, 0.0010914608603716594, -0.083609192741234073, -0.065591861453189335, -0.076544863152496881, -0.046455645665065104, 0.049812299165303581, -0.006296467676917915, -0.062317478872074353, -0.08436608519717019, -0.063260852078023733, 0.086444797087023739, -0.012549057530303302, -0.030034370208016119, -0.0056383003239299798, -0.030570502364304208, 0.043016721245703139, -0.083411742535337674, -0.057532053743057907, -0.064652601793196138, 0.031877238796382337, 0.029299416663846261, 6.5184706102402125e-05, 0.0017386587574764628, -0.086340587256133999, -0.0066584597210612804, -0.0024955512134125883, 0.053519975253802275, -0.039544888458691786, -0.040754270969807112, -0.033033145210067401, -0.037575871127669545, -0.014956853096650832, 0.050157837025622244, -0.1153547980670188, 0.032513467237604003, 0.075793455424502332, 0.019278818714604941, -0.022904223883980154, 0.057304437533482903, -0.075856529795830338, 0.023189429736941592, 0.064688252524816303, -0.036703799384960531, -0.13744933898862527, 0.013994283342905978, -0.071049851346040319, -0.010577297835310279, 0.028334104546130619, -0.097438934245893047, -0.010815883500768404, 0.0039544888458691784, 0.050086535562381886, 0.080164783593930533, 0.04335128965013868, -0.015763108104061053, 0.003455378603186661, -0.0024297344781137949, -0.091660773359453135, -0.081481118299906397, -0.014841673809877944, -0.11980839715557051, -0.023175717917087675, 0.0069991984444310752, -0.0047388049415131349, -0.01317294533365645, 0.011770226162600912, -0.015432653245581693, 0.036561196458479814, 0.044056077190629928, -0.011232722824327432, 0.10229566083814985, 0.03664895210554487, -0.0027204250590168, -0.12890207608268714, -0.10429210180887993, -0.0040312750370511043, 0.087009724065005067, -0.011622138508178627, 0.018450624795428455, -0.011282085375801526, 0.0065268262504636926, 0.090190866271113418, -0.006663944449002846, 0.031926601347856437, -0.0015192696398138176, 0.029691574711668237, -0.038362929587284288, 0.062013762062310122, 0.084486749211884649, -0.007176766511539279, 0.094342805322878975, -0.047652687538311915, 0.015993466677606833, -0.008857835625629298, -0.088254757307740572, 0.092878382962480827, -0.047662285812209652, -9.3240375006624178e-05, 0.037581355855611114, -0.030495087355107673, -0.05934475632774551, 0.1032445187720408, -0.065619285092897162, 0.015253028405495405, 0.036473440811414751, -0.085550786432548473, -0.044113666834016366, 0.11640786583179952, 0.014337078839253859, -0.1489213330694035, 0.085649511535496659, 0.056569483989313052, 0.02065548542793804, -0.037844622796806289, 0.03777332133356593, 0.025087145604723471, -0.024330938739780043, -0.04287960304716399, 0.022303646174378661, -0.022767105685441002, -0.016563878383529709, 0.038393095590962899, 0.076454365141461048, 0.050152352297680676, 0.046372003563956224, 0.045095433135556709, -0.010618433294872024, -0.060809178688143666, -0.041947199297097747, -0.040491004028611943, 0.058927917004186488, -0.045671329569421153, -0.090536404131432088, 0.13829741504658993, -0.17176522494602645, -0.012575152837462784, -0.033369084796488327, -0.017057503898270661, -0.063666721945699611, 0.023759841442864468, 0.065059842842857415, 0.013839339778556732, -0.014216414824539404, -0.031789483149317281, 0.085375275138418347, 0.077839258946706499, -0.026683201435719214, 0.011205299184619601, -0.023688539979624106, 0.10928868896364667, 0.0082627426439693726, -0.13176510406818467, 0.056939703125368765, -0.031751090053726318, 5.4847279415661282e-05, -0.0014548240865004155, -0.055088607445090192, -0.12895229562290211, 0.069710892137305497, 0.042874118319222429, -0.058346535842380469, -0.040905100988200181, -0.10821642465107049, -0.095345824945192878, 0.0026107305001854771, 0.042199496782409793, -0.068251954504848902, -0.0043932670811944689, 0.024582550634099386, 0.030964031594111575, -0.076934278836348083, -0.020326401751444075, 0.005040464978299272, -0.016366428177633327, -0.0020348340663210339, -0.11520122568465496, -0.041892352017682086, 0.026044230630526761, 0.070577479152072936, 0.02071855979926605, -0.048894892718202555, 0.085364305682535224, -0.096509272859797587, 0.025536893295931895, 0.038228553752715916, 0.0060551396474890062, -0.014013479890701458, -0.081695022689627486, -0.069645075402006693, 0.05180325540809208, -0.020096043177898294, -0.017704701795375463, 0.0077087851218711938, 0.0056821781474625088, 0.019010067045468201, -0.061988395195580387, 0.080609046557197389, -0.085912778476691834, 0.067508773868766692, 0.0099026762984976459, 0.027632059369610155, -0.017375618118881494, -0.044272723944321787, -0.064774636989895981, 0.068943030225486229, 0.049444822393218645, 0.032952245472929298, -0.039753308120471294, -0.0052543693680203512, -0.0057150865151119057, 0.063825779056005033, -0.068825108574742555, -0.059640931636590087, -0.038353331313386543, 0.074440098804920876, -0.093876603447845847, -0.018585000629996826, -0.067692512254809156, 0.065915460401741732, -0.032255685024350403, 0.046724397334201848, 0.053399311239087824, 0.026706511529470872, -0.023227822832532552, 0.021620797545653679, 0.071060135210930755, -0.06456621732811646, -0.061275380563176787, -0.055532870408357048, -0.063831263783946601, 0.14611315236332165, -0.060134557151331036, -0.10934353624306233, 0.075447917564183661, 0.087426563388564082, -0.016936839883556206, 0.026488493593793617, 0.080976523329282318, -0.026297899297824195, 0.014326109383370726, -0.0066612020850320627, -0.021977304861855475, -0.095335541080302441, 0.087404624476797824, -0.012280305861166562, 0.015165272758430345, -0.12068595362622109, 0.029485897413859506, 0.075711184505378837, -0.0079857638829202819, -0.11379165060367245, -0.0071630546916853629, -0.030056309119782385, -0.053461014428430445, -0.020556760324989851, -0.071926722225698209, -0.022627245122931062, 0.11952319130260906, 0.03354922382981914, -0.041629085076486917, -0.041903321473565222, -0.097732367190766847, 0.067922870828354937, -0.094929671212626549, 0.10820271283121657, -0.080395142167476313, -0.015137849118722515, -0.0081393362652841354, -0.041596176708837515, 0.055920229319230158, -0.013898300603928569, 0.065388926519351381, 0.021450085388472432, 0.0021280744413276578, 0.013068049911773997, -0.036568395163903117, 0.038074981370352062, 0.0095982738977407247, -0.081727931057276867, -0.1105967965777102, -0.026924529465148124, 0.056712086915793769, 0.033462325171494951, 0.0021332824763374276, -0.069385190910974864, 0.055187090148729107, 0.046026104531280088, 0.02680337439123957, -0.020102084873179608, -0.073066708495557173, -0.028974114168582921, -0.11455869592412009, 0.10903987542918465, -0.035092139999550631, -0.065066007368791687, -0.13660856044960779, 0.067644318254699506, -0.0036957870325845738, 0.023615936443735403, 0.01242601368846546, 0.055832782670831242, -0.098248716857540289, -0.00072060712411398055, -0.028980357052083912, -0.0042451607806714697, 0.10655710295685446, -0.00066352933210495242, -0.079930312984642815, -0.094720595838982249, 0.0056257298748898388, 0.08432530296933799, 0.022332577964032412, -0.06040970811755518, -0.0072310427751437565, -0.014394305672276789, -0.020683564879271581, -0.065082952338294364, -0.098152398083525064, -0.07660826286164861, -0.034548117294464581, -0.013242047746094535, 0.0039932158393816185, 0.057885353581905906, -0.11838647535072555, -0.034795603033253727, 0.017612066196785753, 0.041920070868630623, -0.0026541173284198097, -0.057891150545156818, -0.052565079078314383, 0.032283734264606415, -0.045840601707250746, 0.075114374283881069, -0.10379596476841772, 0.13644089443558124, 0.081077219867824221, 0.043914226226946046, -0.073836143887053882, 0.045551645385205043, -0.064326671594174739, -0.013020871302059551, -0.014261449307771401, 0.040498477111405765, -0.11993471045897043, 0.081578434228903512, -0.041902234058627801, -0.0048585240846434879, -0.021966031518474434, -0.045715744037230992, 0.076940863628169973, -0.06352936618704863, 0.036187320133723853, 0.025121363207973525, -0.0053256255465923708, 0.046489861591353439, 0.022677720237587004, 0.022431572259548067, -0.10868325070919076, -0.036066698706079779, -0.0051512707288147918, -0.078795891868463391, -0.0011326374351791526, 0.015383802707183303, 0.032698440097172009, -6.4212516010156683e-05, 0.0040739274046443855, -0.078596119596431785, -0.051431549802635083, 0.030905840691888471, -0.068632477528855804, 0.013498897810135161, 0.037500109349931504, -0.002766489231437584, -0.15248332135211876, -0.061130315241669168, 0.055017863413827334, -0.034111115449395456, 0.0045091455687132249, 0.00057434528209084599, 0.088591867922012846, 0.023398327361700985, 0.10876218556095714, -0.014597645306308953, 0.078691546529946879, 0.078571148062427834, 0.14787428964738972, 0.012733698661014129, 0.076337979450074603, 0.05425868418808226, 0.010875103058720147, 0.052222891026416501, 0.01178121300686347, -0.0222389347115176, 0.048858589959728103, -0.078439155668406965, 0.01133707643779322, -0.029245233680625808, -0.047269330188476733, 0.12348066828753132, -0.057784129685139893, -0.058764931275170032, 0.033497529185298407, -0.081815663801941024, -0.0088149515033942877, 0.062757032313926472, -0.013077949094068579, -0.030431827466063495, 0.07641646141408702, 0.11792361013115232, -0.1245794157837051, 0.05051384592798993, 0.067687126598706265, 0.002229601250352663, -0.0016409865202595599, -0.073644621139648592, 0.049225470845473643, -0.023680148959745562, -0.092722873118666255, 0.03596257632768831, -0.025977530088108946, -0.064396235153185752, 0.00181712501903742, 0.043272101066844482, -0.021197265007352837, -0.0024045134684428293, 0.040152442997351032, -0.0044217451996994006, -0.048646220440632014, -0.012269941600940774, -0.042547926580729936, 0.031105585093904438, 0.031127017135923456, -0.10535133460066373, 0.10013585135583879, -0.10612188479278563, -0.023901325403780545, -0.027812045996899116, 0.014226639658250271, -0.034985119139533699, 0.068550428202842836, 0.023555291289725812, -0.03493160870952524, -0.060880599901629667, -0.038085156718024045, 0.061743901505766224, 0.051840904592199831, -0.066174565110467029, 0.023143260978660641, -0.034788245349127563, 0.064505039694202959, -0.047403106263497891, -0.049290240761796382, 0.077911186092323448, -0.056064661200867914, 0.01167240846584626, -0.13363338054113719, -0.07000279045732255, -0.037797984076978625, -0.050834908508040712, 0.023783602457761923, 0.010689600234690805, -0.061793844573774126, -0.10758628689401724, -0.079438017028564956, 0.054018779093544314, 0.083940027873277048, -0.079721622307609805, -0.014166886344740819, -0.0020035196835669028, 0.064633464726223264, -0.068008189178757061, 0.078225113948373101, 0.052172669258252306, -0.041112955216002958, -0.0073487657211623756, -0.091608072493489928, -0.009332218993476105, 0.027069142860281611, 0.011158708337765007, -0.018173925711874622, -0.0013484628362132904, 0.040978287300481663, -0.064476500798198444, -0.012171839145925257, 0.044005193957960431, -0.045255554339158205, -0.034546333613464299, -0.054566369160630922, -0.017487208526766006, 0.057816235943144965, -0.042144814674666173, -0.023145044659660923, -0.092109286854569206, -0.005351488921096461, 0.0035241077363074185, 0.0056292972368904032, 0.0054081207928554191, -0.028692292570538347, 0.12537315382883066, 0.026344968374167065, -0.002301840330864089, -0.034508876312458373, -0.010067987406092483, -0.046225876803311687, -0.033213923906253551, -0.013290207133102153, 0.048076445841104397, 0.012007740493899299, -0.019948688307155342, 0.038422272427077367, -0.035659350557640343, 0.029516353192668689, -0.03852750960609401, 0.03875225341212956, 0.081635512020912543, -0.0073987087891702764, 0.0772703986929721, 0.0045091455687132249, 0.009603338505518989, 0.02972749643107709, 0.0057699850757876557, 0.016263603360572462, -0.0097745718815460732, -0.018068688532857979, -0.0033711570905332259, 0.037428762109920223, 0.036515517437775771, -0.087773158342883351, 0.0079124089172515292, 0.021695803846931691, 0.092100368449567802, -0.085268870218487239, -0.063395590112027472, 0.10256745447947339, 0.053232175772419892, 0.022791875821605058, 0.025790243583079322, 0.10900074592724097, 0.0023972672643791828, 0.089826175174208073, -0.032284626105106559, 0.017934912457836818, -0.097488868751420113, 0.027033469240275963, 0.00078771812174959573, 0.029173886440614524, -0.062942535137955807, 0.024547017925882673, 0.079401451568059167, -0.074978814527859622, 0.035539843930621443, -0.044959463293111374, -0.049836047147882714, -0.11175831675367714, 0.12190567796428219, -0.036310394122743325, 0.027422311698337469, 0.05383149258851469, 0.020775870371036183, -0.025183792042983398, 0.01210049190591397, -0.077377865473239091, 0.047774111911556574, -0.1097374061803575, 0.1132994171379209, -0.065766994001902571, 0.038119046657029407, 0.046457755333348359, -0.067469517516671851, 0.027482956852347064, -0.012214647489932026, 0.1001643902518433, -0.015924703970518858, 0.01661320483662776, -0.040082879438340026, -0.06556097884636998, 0.073409175247611355, -0.038495403348088937, -0.065111491234298879, -0.09225911605859291, -0.0227579858825997, 0.071828833881361381, 0.032512937273142667]\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mini_stances = train_stances[:1]\n",
    "for (title, body_id, stance) in mini_stances:\n",
    "        body = id2body[body_id]\n",
    "        features = extract_features(title, body, idf)\n",
    "        print(title)\n",
    "        print(body[:25])\n",
    "        print(features)\n",
    "        print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier\n",
    "\n",
    "Here, we've written a function to train a classifier for you. We're going to use the xgboost algorithm, which combines different decision trees using a technique known as gradient boosting. This algorithm is very powerful and flexible, which is why we've chosen it! Take a look at some of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_relatedness_classifier(trainX, trainY):\n",
    "    xg_train = xgb.DMatrix(trainX, label=trainY)\n",
    "    # setup parameters for xgboost\n",
    "    param = {}\n",
    "    # use softmax multi-class classification\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 20\n",
    "\n",
    "    num_round = 1000\n",
    "    relatedness_classifier = xgb.train(param, xg_train, num_round);\n",
    "\n",
    "    return relatedness_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the final function\n",
    "\n",
    "Look familiar? Here's the final form of the function we had you fill in the skeleton for before. There are a few changes, a few tasks we didn't anticipate. Notice how we break the training up into several parts, actually developing a new classifier for each. We're going to train on the first 1000 examples of each, in the interest of time. If we trained on everything, how might that impact our accuracy? \n",
    "\n",
    "There's nothing here for you to do, but you should understand what each line is doing. Read through the code and stick your hand up whenever you're confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions():\n",
    "    \n",
    "    # Load and clean the training body using your function\n",
    "    id2body, id2body_sentences = load_body(train_body_path)\n",
    "    # Load and clean the test body using your function\n",
    "    test_id2body, test_id2body_sentences = load_body(test_body_path)\n",
    "    # We're going to merge them together --- can you figure out why?\n",
    "    id2body.update(test_id2body)\n",
    "    id2body_sentences.update(test_id2body_sentences)\n",
    "    \n",
    "    # Prepare the idf\n",
    "    idf = prepare_idf(id2body)\n",
    "    \n",
    "    # Load and clean the headline-article-stance sets for training using the load_stances function\n",
    "    train_stances = load_stance(train_stance_path)\n",
    "    \n",
    "    # Load and clean the headline-article-stance sets for testing using the load_test function\n",
    "    test = load_test(test_headline_path)\n",
    "\n",
    "    # Here, we're going to create the training set for agree / disagree\n",
    "    # First step, extract features from each example in training set and add that vector to train_x\n",
    "    train_x = []\n",
    "    for (title, body_id, stance) in train_stances[:1000]:\n",
    "        body = id2body[body_id]\n",
    "        body_sentences = id2body_sentences[body_id]\n",
    "        train_x.append(extract_features(title, body, body_sentences))\n",
    "    \n",
    "    # Next step: make train_y a binary label vector that describes whether an example is unrelated\n",
    "    train_y = []\n",
    "    for (clean_title, body_id, stance) in train_stances[:1000]:\n",
    "        train_y.append(int(stance == 'unrelated'))\n",
    "        \n",
    "    # And now we train our unrelated-related classifier!\n",
    "    relatedness_classifier = train_relatedness_classifier(train_x, train_y)\n",
    "\n",
    "    # Now, we're creating a training set from all of the related data for training our other three categories\n",
    "    related_train_x = []\n",
    "    for i in range(len(train_x)): \n",
    "        # Check if the label in train_y was \"related\"\n",
    "        if train_y[i] == 0:\n",
    "            related_train_x.append(train_x[i])\n",
    "    \n",
    "    # Now, we're going to create a label vector for the discuss label\n",
    "    related_train_y = []\n",
    "    for i in range(len(train_x)):\n",
    "        if train_y[i] == 0:\n",
    "            related_train_y.append(int(train_stances[i][2] == 'discuss'))\n",
    "            \n",
    "    # Aaaand train our classifier\n",
    "    discuss_classifier = train_relatedness_classifier(related_train_x, related_train_y)\n",
    "\n",
    "    # Last one! Here, we're going to create a dataset of agree/disagree pairs\n",
    "    agree_train_x = []\n",
    "    for i in range(len(train_x)):\n",
    "        # Check if stance is agree or disagree\n",
    "        if train_stances[i][2] == 'agree' or train_stances[i][2] == 'disagree':\n",
    "            agree_train_x.append(train_x[i]) \n",
    "    \n",
    "    # And here, we make a label vector for agree, from the agree/disagree pairs we've seen\n",
    "    agree_train_y = []\n",
    "    for i in range(len(train_x)):\n",
    "        if train_stances[i][2] == 'agree' or train_stances[i][2] == 'disagree':\n",
    "            agree_train_y.append(int(train_stances[i][2] == 'agree'))\n",
    "    \n",
    "    # And train our agree/disagree classifier\n",
    "    agree_classifier = train_relatedness_classifier(agree_train_x, agree_train_y)\n",
    "\n",
    "    # Now, let's create our test set, just like we created our training set\n",
    "    test_x = []\n",
    "    for (title, body_id) in test[:1000]:\n",
    "        body = id2body[body_id]\n",
    "        body_sentences = id2body_sentences[body_id]\n",
    "        test_x.append(extract_features(title, body, body_sentences))\n",
    "\n",
    "    # Don't worry about this --- it's creating a matrix for the classifier    \n",
    "    xg_test = xgb.DMatrix(test_x)\n",
    "    # Now, we run each classifier on the full test set\n",
    "    relatedness_pred = relatedness_classifier.predict(xg_test);\n",
    "    discuss_pred = discuss_classifier.predict(xg_test)\n",
    "    agree_pred = agree_classifier.predict(xg_test)\n",
    "\n",
    "    ret, scores = [], []\n",
    "    # We're going to loop through the three predictions for each example together\n",
    "    for (pred_relate, pred_discuss, pred_agree) in zip(relatedness_pred, discuss_pred, agree_pred):\n",
    "        scores.append((pred_relate, pred_discuss, pred_agree))\n",
    "        # Now we pick out a prediction! For each one, we're going to choose it as the prediction if the classifier\n",
    "        # Predicted over .5 probability that this is the correct label. If none of them are, we predict disagree.\n",
    "        # Do you have an idea for a better method? How might we improve this solution?\n",
    "        if pred_relate >= 0.5:\n",
    "            ret.append('unrelated')\n",
    "        elif pred_discuss >= 0.5:\n",
    "            ret.append('discuss')\n",
    "        elif pred_agree >= 0.5:\n",
    "            ret.append('agree')\n",
    "        else:\n",
    "            ret.append('disagree')\n",
    "    return ret, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And we're done! Let's test it out. This should run without causing any errors --- then there's a good chance it's all working.\n",
    "pred, scores = make_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "And finally, we're going to score our attempt. We've imported a scoring function for you, since it would be pretty time-consuming to write it yourself. Definitely take a look, but this code isn't as important to understand. Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Headline', 'Body ID', 'Stance']\n",
      "CONFUSION MATRIX:\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    21     |     0     |    36     |     4     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     5     |     0     |    15     |     8     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    40     |     0     |    118    |    14     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     5     |     0     |     5     |    729    |\n",
      "-------------------------------------------------------------\n",
      "ACCURACY: 0.868\n",
      "\n",
      "MAX  - the best possible score (100% accuracy)\n",
      "NULL - score as if all predicted stances were unrelated\n",
      "TEST - score based on the provided predictions\n",
      "\n",
      "||    MAX    ||    NULL   ||    TEST   ||\n",
      "|| 11651.25  ||  4587.25  ||  345.25   ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "FIELDNAMES = ['Headline', 'Body ID', 'Stance']\n",
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "USAGE = \"\"\"\n",
    "FakeNewsChallenge FNC-1 scorer - version 1.0\n",
    "Usage: python scorer.py gold_labels test_labels\n",
    "\n",
    "  gold_labels - CSV file with reference GOLD stance labels\n",
    "  test_labels - CSV file with predicted stance labels\n",
    "\n",
    "The scorer will provide three scores: MAX, NULL, and TEST\n",
    "  MAX  - the best possible score (100% accuracy)\n",
    "  NULL - score as if all predicted stances were unrelated\n",
    "  TEST - score based on the provided predictions\n",
    "\"\"\"\n",
    "\n",
    "ERROR_MISMATCH = \"\"\"\n",
    "ERROR: Entry mismatch at line {}\n",
    " [expected] Headline: {} // Body ID: {}\n",
    " [got] Headline: {} // Body ID: {}\n",
    "\"\"\"\n",
    "\n",
    "SCORE_REPORT = \"\"\"\n",
    "MAX  - the best possible score (100% accuracy)\n",
    "NULL - score as if all predicted stances were unrelated\n",
    "TEST - score based on the provided predictions\n",
    "\n",
    "||    MAX    ||    NULL   ||    TEST   ||\\n||{:^11}||{:^11}||{:^11}||\n",
    "\"\"\"\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g['Stance'], t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "\n",
    "    return score, cm\n",
    "\n",
    "\n",
    "def score_defaults(gold_labels):\n",
    "    \"\"\"\n",
    "    Compute the \"all false\" baseline (all labels as unrelated) and the max\n",
    "    possible score\n",
    "    :param gold_labels: list containing the true labels\n",
    "    :return: (null_score, best_score)\n",
    "    \"\"\"\n",
    "    unrelated = [g for g in gold_labels if g['Stance'] == 'unrelated']\n",
    "    null_score = 0.25 * len(unrelated)\n",
    "    max_score = null_score + (len(gold_labels) - len(unrelated))\n",
    "    return null_score, max_score\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data = None\n",
    "    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        print(reader.fieldnames)\n",
    "        if reader.fieldnames != FIELDNAMES:\n",
    "            error = 'ERROR: Incorrect headers in: {}'.format(filename)\n",
    "            raise FNCException(error)\n",
    "        else:\n",
    "            data = list(reader)\n",
    "    return data\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = ['CONFUSION MATRIX:']\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    lines.append(\"ACCURACY: {:.3f}\".format(hit / total))\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "\n",
    "\n",
    "gold_labels = load_dataset(test_headline_path)\n",
    "#test_labels = load_dataset(test_filename)\n",
    "\n",
    "test_score, cm = score_submission(gold_labels, pred)\n",
    "null_score, max_score = score_defaults(gold_labels)\n",
    "print_confusion_matrix(cm)\n",
    "print(SCORE_REPORT.format(max_score, null_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END\n",
    "\n",
    "Great work, you guys. We appreciate your hard work and patience so much. It's not easy to tackle a challenge of this size as a beginning coder, and you guys did so much great work. We couldn't be prouder :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
